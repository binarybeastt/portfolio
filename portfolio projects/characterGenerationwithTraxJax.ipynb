{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install -q -U trax"
      ],
      "metadata": {
        "id": "J2JG4Al2YqSI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AviY68G4Xtvu"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import trax\n",
        "import trax.fastmath.numpy as np\n",
        "import pickle\n",
        "import numpy\n",
        "import random as rnd\n",
        "\n",
        "from trax import fastmath\n",
        "from trax import layers as tl\n",
        "\n",
        "#trax.supervised.trainer_lib.init_random_number_generators(32)\n",
        "rnd.seed(32)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing the data"
      ],
      "metadata": {
        "id": "sPQKjxZEcZvK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dirname = '/content/data/'\n",
        "lines = []\n",
        "for filename in os.listdir(dirname):\n",
        "  with open(os.path.join(dirname, filename)) as files:\n",
        "    for line in files:\n",
        "      pure_line = line.strip()\n",
        "      if pure_line:\n",
        "        lines.append(pure_line)"
      ],
      "metadata": {
        "id": "kUTRK_e_ZIy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_lines = len(lines)\n",
        "print(f'number of lines: {n_lines}')\n",
        "print(f'Sample line at position 0 {lines[0]}')\n",
        "print(f'Sample line at position 999 {lines[999]}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQPIfGl9bv5f",
        "outputId": "cea7b3f1-a20c-42b2-ea01-28936b4a9fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "number of lines: 125097\n",
            "Sample line at position 0 HAMLET\n",
            "Sample line at position 999 That you, at such times seeing me, never shall,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i, line in enumerate(lines):\n",
        "  #convert all to lowercase\n",
        "  lines[i] = line.lower()\n",
        "\n",
        "print(f\"Number of lines: {n_lines}\")\n",
        "print(f\"Sample line at position 0 {lines[0]}\")\n",
        "print(f\"Sample line at position 999 {lines[999]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JXpu2cifcS_x",
        "outputId": "7147464b-d207-4880-f614-250c0a97fdd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines: 125097\n",
            "Sample line at position 0 hamlet\n",
            "Sample line at position 999 that you, at such times seeing me, never shall,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_lines = lines[-1000:] # Create a holdout validation set\n",
        "lines = lines[:-1000] # Leave the rest for training\n",
        "\n",
        "print(f\"Number of lines for training: {len(lines)}\")\n",
        "print(f\"Number of lines for validation: {len(eval_lines)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwpH85H5hCos",
        "outputId": "c3b0072c-7743-42f6-e1c9-48250beb89d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines for training: 124097\n",
            "Number of lines for validation: 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def line_to_tensor(line, EOS_int=1):\n",
        "  tensor = []\n",
        "  for c in line:\n",
        "    tensor.append(ord(c))\n",
        "  \n",
        "  tensor.append(EOS_int)\n",
        "  return tensor"
      ],
      "metadata": {
        "id": "3j8I4R6VcrEU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "line_to_tensor('abc xyz')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXB93JYXdzpk",
        "outputId": "5ddf79a9-8143-4eb3-e875-256c6ba98a96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[97, 98, 99, 32, 120, 121, 122, 1]"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\n",
        "# GRADED FUNCTION: data_generator\n",
        "def data_generator(batch_size, max_length, data_lines, line_to_tensor=line_to_tensor, shuffle=True):\n",
        "    \"\"\"Generator function that yields batches of data\n",
        "\n",
        "    Args:\n",
        "        batch_size (int): number of examples (in this case, sentences) per batch.\n",
        "        max_length (int): maximum length of the output tensor.\n",
        "        NOTE: max_length includes the end-of-sentence character that will be added\n",
        "                to the tensor.  \n",
        "                Keep in mind that the length of the tensor is always 1 + the length\n",
        "                of the original line of characters.\n",
        "        data_lines (list): list of the sentences to group into batches.\n",
        "        line_to_tensor (function, optional): function that converts line to tensor. Defaults to line_to_tensor.\n",
        "        shuffle (bool, optional): True if the generator should generate random batches of data. Defaults to True.\n",
        "\n",
        "    Yields:\n",
        "        tuple: two copies of the batch (jax.interpreters.xla.DeviceArray) and mask (jax.interpreters.xla.DeviceArray).\n",
        "        NOTE: jax.interpreters.xla.DeviceArray is trax's version of numpy.ndarray\n",
        "    \"\"\"\n",
        "    # initialize the index that points to the current position in the lines index array\n",
        "    index = 0\n",
        "    \n",
        "    # initialize the list that will contain the current batch\n",
        "    cur_batch = []\n",
        "    \n",
        "    # count the number of lines in data_lines\n",
        "    num_lines = len(data_lines)\n",
        "    \n",
        "    # create an array with the indexes of data_lines that can be shuffled\n",
        "    lines_index = [*range(num_lines)]\n",
        "    \n",
        "    # shuffle line indexes if shuffle is set to True\n",
        "    if shuffle:\n",
        "        rnd.shuffle(lines_index)\n",
        "    \n",
        "    ### START CODE HERE (Replace instances of 'None' with your code) ###\n",
        "    while True:\n",
        "        \n",
        "        # if the index is greater or equal than to the number of lines in data_lines\n",
        "        if index >= num_lines:\n",
        "            # then reset the index to 0\n",
        "            index = 0\n",
        "            # shuffle line indexes if shuffle is set to True\n",
        "            if shuffle:\n",
        "                rnd.shuffle(lines_index)\n",
        "            \n",
        "        # get a line at the `lines_index[index]` position in data_lines\n",
        "        line = data_lines[lines_index[index]]\n",
        "        \n",
        "        # if the length of the line is less than max_length\n",
        "        if len(line) < max_length:\n",
        "            # append the line to the current batch\n",
        "            cur_batch.append(line)\n",
        "            \n",
        "        # increment the index by one\n",
        "        index += 1\n",
        "        \n",
        "        # if the current batch is now equal to the desired batch size\n",
        "        if len(cur_batch) == batch_size:\n",
        "            \n",
        "            batch = []\n",
        "            mask = []\n",
        "            \n",
        "            # go through each line (li) in cur_batch\n",
        "            for li in cur_batch:\n",
        "                # convert the line (li) to a tensor of integers\n",
        "                tensor = line_to_tensor(li)\n",
        "                \n",
        "                # Create a list of zeros to represent the padding\n",
        "                # so that the tensor plus padding will have length `max_length`\n",
        "                pad = [0] * (max_length - len(tensor))\n",
        "                \n",
        "                # combine the tensor plus pad\n",
        "                tensor_pad = tensor + pad\n",
        "                \n",
        "                # append the padded tensor to the batch\n",
        "                batch.append(tensor_pad)\n",
        "\n",
        "                # A mask for  tensor_pad is 1 wherever tensor_pad is not\n",
        "                # 0 and 0 wherever tensor_pad is 0, i.e. if tensor_pad is\n",
        "                # [1, 2, 3, 0, 0, 0] then example_mask should be\n",
        "                # [1, 1, 1, 0, 0, 0]\n",
        "                # Hint: Use a list comprehension for this\n",
        "                example_mask = [0 if t == 0 else 1 for t in tensor_pad]\n",
        "                mask.append(example_mask)\n",
        "               \n",
        "            # convert the batch (data type list) to a trax's numpy array\n",
        "            batch_np_arr = np.array(batch)\n",
        "            mask_np_arr = np.array(mask)\n",
        "            \n",
        "            ### END CODE HERE ##\n",
        "            \n",
        "            # Yield two copies of the batch and mask.\n",
        "            yield batch_np_arr, batch_np_arr, mask_np_arr\n",
        "            \n",
        "            # reset the current batch to an empty list\n",
        "            cur_batch = []\n",
        "            "
      ],
      "metadata": {
        "id": "u6-yjNzCd4Tw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "\n",
        "infinite_data_generator = itertools.cycle(\n",
        "    data_generator(batch_size=2, max_length=10, data_lines=tmp_lines))"
      ],
      "metadata": {
        "id": "HNA8kdQMe0iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ten_lines = [next(infinite_data_generator) for _ in range(10)]\n",
        "print(len(ten_lines))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA-FaL5wfD84",
        "outputId": "de830948-5ec6-4ff7-e927-61bac53127ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def GRULM(vocab_size=256, d_model=512, n_layers=2, mode='train'):\n",
        "    model = tl.Serial(\n",
        "      tl.ShiftRight(mode=mode), # Stack the ShiftRight layer\n",
        "      tl.Embedding(vocab_size=vocab_size, d_feature=d_model), # Stack the embedding layer\n",
        "      [tl.GRU(n_units=d_model) for _ in range(n_layers)], # Stack GRU layers of d_model units keeping n_layer parameter in mind (use list comprehension syntax)\n",
        "      tl.Dense(n_units=vocab_size), # Dense layer\n",
        "      tl.LogSoftmax() # Log Softmax\n",
        "    )\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "U_5WsSUUfFwR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GRULM()\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3ldM1IkgXY3",
        "outputId": "9105e170-969c-4ec3-8c15-2273a5da489d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Serial[\n",
            "  Serial[\n",
            "    ShiftRight(1)\n",
            "  ]\n",
            "  Embedding_256_512\n",
            "  GRU_512\n",
            "  GRU_512\n",
            "  Dense_256\n",
            "  LogSoftmax\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "max_length = 64"
      ],
      "metadata": {
        "id": "wnbYm6-Mgbtu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def n_used_lines(lines, max_length):\n",
        "    '''\n",
        "    Args: \n",
        "    lines: all lines of text an array of lines\n",
        "    max_length - max_length of a line in order to be considered an int\n",
        "    output_dir - folder to save your file an int\n",
        "    Return:\n",
        "    number of efective examples\n",
        "    '''\n",
        "\n",
        "    n_lines = 0\n",
        "    for l in lines:\n",
        "        if len(l) <= max_length:\n",
        "            n_lines += 1\n",
        "    return n_lines\n",
        "\n",
        "num_used_lines = n_used_lines(lines, 32)\n",
        "print('Number of used lines from the dataset:', num_used_lines)\n",
        "print('Batch size (a power of 2):', int(batch_size))\n",
        "steps_per_epoch = int(num_used_lines/batch_size)\n",
        "print('Number of steps to cover one epoch:', steps_per_epoch)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JiWvwdqYh7GA",
        "outputId": "7c6e6cbe-f187-454b-a972-64375edb72ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of used lines from the dataset: 25927\n",
            "Batch size (a power of 2): 32\n",
            "Number of steps to cover one epoch: 810\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from trax.supervised import training\n",
        "def train_model(model, data_generator, batch_size=32, max_length=64, eval_lines=eval_lines, n_steps=100, output_dir='/content/model2'):\n",
        "  bare_train_generator = data_generator(batch_size, max_length, data_lines=lines)\n",
        "  infinite_train_generator = itertools.cycle(bare_train_generator)\n",
        "\n",
        "  bare_eval_generator = data_generator(batch_size, max_length, data_lines=eval_lines)\n",
        "  infinite_eval_generator = itertools.cycle(bare_eval_generator)\n",
        "\n",
        "  train_task = training.TrainTask(\n",
        "      labeled_data = infinite_train_generator,\n",
        "      loss_layer = tl.CrossEntropyLoss(),\n",
        "      optimizer = trax.optimizers.Adam(0.0005)\n",
        "  )\n",
        "\n",
        "  eval_task = training.EvalTask(\n",
        "      labeled_data = infinite_eval_generator,\n",
        "      metrics = [tl.CrossEntropyLoss(), tl.Accuracy()],\n",
        "      n_eval_batches=3\n",
        "  )\n",
        "\n",
        "  training_loop = training.Loop(model,\n",
        "                                train_task,\n",
        "                                eval_tasks=[eval_task],\n",
        "                                output_dir=output_dir)\n",
        "  training_loop.run(n_steps=n_steps)\n",
        "  return training_loop"
      ],
      "metadata": {
        "id": "QHDtcVkSh754"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(GRULM(), data_generator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACvRXVhskZIt",
        "outputId": "b3bc323b-200e-410f-fa6d-c84a77289d43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/trax/layers/base.py:851: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip.GzipFile(fileobj=f, compresslevel=compresslevel) as gzipf:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Step      1: Total number of trainable weights: 3411200\n",
            "Step      1: Ran 1 train steps in 7.01 secs\n",
            "Step      1: train CrossEntropyLoss |  5.54509401\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/trax/supervised/training.py:1249: FutureWarning: GzipFile was opened for writing, but this will change in future Python releases.  Specify the mode argument for opening it for writing.\n",
            "  with gzip_lib.GzipFile(fileobj=f, compresslevel=2) as gzipf:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Step      1: eval  CrossEntropyLoss |  5.54105727\n",
            "Step      1: eval          Accuracy |  0.14964740\n",
            "\n",
            "Step    100: Ran 99 train steps in 177.33 secs\n",
            "Step    100: train CrossEntropyLoss |  3.37116504\n",
            "Step    100: eval  CrossEntropyLoss |  2.91466570\n",
            "Step    100: eval          Accuracy |  0.18991873\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<trax.supervised.training.Loop at 0x7f01b7fb5670>"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    }
  ]
}